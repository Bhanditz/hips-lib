\documentclass{article}

\usepackage{nips12submit_e,times}
\usepackage{amsmath,amssymb}
\usepackage{macros}
\usepackage{graphicx} 
\usepackage{caption}
\usepackage{subfig}


\title{Bayesian Program Induction}
\author{Eyal Dechter}

\begin{document}
\maketitle

\begin{asbtract}
  Humans poses a rich, generative knowledge of the processes which
  give rise to the data they observe in the world, and a great deal of
  research has shown that this generative knowledge is used to infer
  the hidden states of the world that give rise to observations. One
  popular way to think about this knowledge is as theories expressed
  in a probabilistic language of thought: such theories, as formalized
  in stochastic programming languages like Church, enable inference in
  domains where traditional stastical models are not adequately
  expressive. We do not yet possess, however, a satisfying model of
  how these theories are acquired. Although several domain specific
  proposals exist and motivate a broader generalization, what is
  missing from previous models is an adequate emphasis on higher-order
  function abstraction, that is, the ability to remove redundancy from
  a program by abstracting a way a pattern within that program. This
  idea is central to functional programming techniques. Here, we
  provide a generative model of theories in a probabilistic language
  of thought that renders abstraction a fundamental part of the model
  generating process.
\end{abstract}

\section{Introduction}
Building on previous work, we formulate a statement in the language of
thought as a lambda expression in lambda calculus. In practice, we
implemenet this model using a Scheme-like programming language. In the
past, researchers have placed prior distributions over statements in
the lambda calculus by expressing a subset of the language a either a
context-free grammar or a mildly context-sensitive grammar and
assigning probabilities to the transition probabilities of the
productions in that grammar (CITE: Goodman, Piandtadosi). This models
are similar to adaptor grammar and fragment grammars, which are
probabilistic grammars used to put proirs over parse trees for
sentences in a natural language. 

Commonly, function definitions can be used to eliminate redundancy in
a program. If you need to calculuate the maximum value in a list at
multiple points in a program, you might decide that writing a function
that performs this task once and calling it multiple times in your
program, is a more efficient use of resources. This is of course true. 

In the adaptor or fragment grammar implementation of this idea, you
cache part of the derivation tree, and store the production of that
entire part of the derivation tree as a new production with increased
probability. But this is not exactly what we want. Suppose that I am
doing symbolic regression and I am constantly coming across
expressions of the form $f(x) + f(x)^2$ for various functions $f$,
then I would want to add a production rule for that kind of
pattern. In general, this may or may not be possible in a CFG, but it
will inevitably require seemingly redundant production rules. In the
classical theory of grammars, redundancy is not much of a concern, but
once we start placing probability distributions over production rules,
it becomes of paramount importance that our production rule set is
itself a parsimonious representation of the grammar.

To deal with the limitations of the grammar representation, we shift
gears and instead consider an interpretation of programs that is more
closely related to functional programming: each potential program will
be regarded as a composition of functions. Functions are either
generated from a given probabilistic CFG, or from a library of
previously generated functions. 



\bibliographystyle{plain} 
\bibliography{library}

\end{document}
