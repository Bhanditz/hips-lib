\documentclass{article}

\usepackage{nips12submit_e,times}
\usepackage{amsmath,amssymb}
\usepackage{macros}
\usepackage{graphicx} 
\usepackage{caption}
\usepackage{subfig}


\title{Discovering Sparse Networks in Spiking Data}

\begin{document}
\maketitle

\begin{abstract}
  In order to reason about the interacting processes giving rise to a
  given set of data, we must understand the causal relationships
  between those latent processes. One way to uncover these
  relationships is to discover a directed network structure from the
  data. Often, these data have the form of discrete events --- for
  example, neural spikes --- which can be modeled effectively via
  interacting point processes; the Hawkes process is the classical
  example of such a joint process. Here we provide a fully-Bayesian
  treatment of the Hawkes process, introducing 1)~convenient conjugate
  priors for the model parameters and 2)~a sparsity-promoting
  spike-and-slab prior on the elements of the interaction matrix.  We
  demonstrate how to perform efficient inference in this model with
  Markov chain Monte Carlo.  This enables us to both recover posterior
  samples of the network structure and infer the characteristics of
  the underlying temporal dynamics.  We validate our approach on a
  simulated dataset with known ground truth before examining two
  real-world data sets --- neural spike train data and
  financial tick streams.  In each case, we uncover sparse networks
  with meaningful parameters, suggesting that this method is widely
  applicable for network discovery.
\end{abstract}

\section{Introduction}
Many real-world phenomena are events in time --- for example, neural
spikes, stock price movements and website hits --- and are best
modelled via point processes.  Where multiple streams of events are
present, their relationships can be captured by networks of
\emph{interacting} point processes.  In these networks, the processes
themselves are the nodes and interactions are captured by weighted
directed edges between the processes.  An event on one node influences
the probability of subsequent events on that and other nodes.  One
popular model for such interaction is the multivariate Hawkes process
\cite{hawkes-1971b,hawkes-oakes-1974a}, in which an event at one node
induces a transient additive increase in the Poisson intensity of
another node. While univariate and bivariate Hawkes processes have
been widely applied in, e.g., seismology \cite{Alberta1992} and
finance \cite{Bacry_modelingmicrostructure}, the multivariate case is
less well studied due to its complexity. It has, however, received
some previous attention in the machine learning literature
\cite{Simma2010}.

Here, we introduce sparsity-inducing priors over multivariate Hawkes
processes: a spike-and-slab prior~\cite{Mitchell1988}
over the interaction weights expresses the intuitive belief that
real-world interaction networks are sparsely connected. That is, most
nodes do not influence each other directly, but if two nodes do
influence one another then that influence is substantial.

Attempts to model data with the multivariate Hawkes process have fit
models by searching for the maximum likelihood or MAP settings of the
parameters \cite{Simma2010, Alberta1992, Ozaki1979a}.  However, a
fully Bayesian treatment of these models, that provides samples from
the posterior, has not been given and tested.  Here we provide such a
treatment. To do so we present a convenient conjugate prior
formulation of the multivariate Hawkes process, enabling efficient
Markov chain Monte Carlo algorithms using Gibbs sampling. We
demonstrate that this algorithm allows us to recover network structure
for large networks.

The remainder of this paper is structured as follows. We first derive
our model and show that an appropriate choice of priors and temporal
kernels renders the model conjugate in all its parameters except for
the adjacency structure. We then validate the model against simulated
datasets of~$10$ and~$100$ nodes. Finally, we demonstrate the model's
suitability for real-world problems by fitting it to two datasets, neural spikes from cat visual cortex and
intraday price movements of stocks in the S\&P 100.

\section{A Sparse Prior for the Multivariate Hawkes Process}
We consider a network of~$K$ nodes where each node produces events in
the time interval~$[0,T]$.  Our data consist of a set of~$N$ events
across these nodes, where each event is indexed by~$n$ in increasing
chronological order, i.e.,~${n=1}$ is the first and~${n=N}$ is the
last.  We denote the time of the~$n$th event by~${s_n\in[0,T]}$ and
the index of the~$n$th event's node by~$c_n\in\{1,2,\cdots,K\}$. The
Hawkes process is a model for these events that can be thought of as
fertility dynamics on a network. This is shown schematically in
Figure~\ref{fig:fertility}: the basic intuition is that an event on
one node induces a Poisson process on that node and on each other
connected node according to a weight matrix.  For example, in a
network of purely excitatory neurons, each spike on a presynaptic
neuron induces Poisson processes on its postsynaptic neurons.  The
magnitude of an induced Poisson process is determined by the weight of
the directed edge from the presynaptic to the postsynaptic neuron,
i.e., the strength of the synapse.  A parsimonious explanation of the
data demands that we use as few interactions as possible; in a
Bayesian context we can express this belief with a prior that enforces
sparsity in the weight matrix.
 
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.5\linewidth]{figures/Hawkes} 
%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{Fertility dynamics of the multivariate Hawkes process.}
\label{fig:fertility}
\end{figure}


\subsection{The Hawkes Process Likelihood}
The Hawkes process assumes that each node~$k$ has a base Poisson
rate~${\lambda_0^{(k)}>0}$ and that each node influences every other
node (including itself) according to a some temporal kernel from
a~$\theta$-indexed family of nonnegative ``excitatory
kernels''~$g_\theta(t)$ satisfying two
properties:~1)~${g_\theta(t)=0}$ for all~${t< 0}$, and
2)~${\int^\infty_0g_\theta(t)\,\mathrm{d}t=1}$.  We let~$\bW$ denote a
non-negative weight matrix, each of whose elements~$W_{k, k'}$
corresponds to the magnitude of node $k$'s excitatory influence on
node $k'$.  The magnitude of the eigenvalues of~$\bW$ must be less than one
\cite{Ogata1981}.

The superposition theorem for Poisson processes \cite{kingman-poisson-processes}
leads to a tractable latent variable representation for the Hawkes
process.  In this representation, an event on a node is explained by
either that node's base Poisson process or by the Poisson process
induced by exactly one previous event.  To exploit this representation
for inference, we introduce a latent~${N \times N}$ binary
matrix~$\bZ$: if~${Z_{n,n'}=1}$, then event~$n$ induced event~$n'$.
Note that~${Z_{n,n'}=0}$ for~${n'\leq n}$ and that there is at most
one nonzero entry in each column.  If the entire column is zeros, then
that event was induced by the background process.

Having introduced these auxiliary variables, we can now write the
joint distribution over the event times, the associated nodes, and the
latent causes, given~$\{\lambda^{(k)}_0\}^K_{k=1}$,~$\theta$,
and~$\bW$ \cite{daley2008pointprocessesII}:
\begin{multline}
  p(\{s_n,c_n\}^N_{n=1},\bZ\given \{\lambda^{(k)}_0\}^K_{k=1}, \theta, \bW) =
  \prod^K_{k=1}
  \exp\left\{ -\lambda^{(k)}_0 T \right\} \,
  \prod^N_{n=1}
   (\lambda^{(k)}_0)^{
     \bbI( c_n = k ) \bbI( \sum_{n'}Z_{n',n} = 0)
    }\\
  \times
%  \prod_{n=1}^N
  \exp\left\{
  - W_{c_n,k} \int^T_{s_n} \,g_\theta(\tau - s_n)\,\mathrm{d}\tau
  \right\}
  \prod^N_{n'=1}
  \left[
    W_{c_n,c_{n'}}
    \,
    g_\theta(s_{n'} - s_n)
    \right]^{Z_{n,n'}},
\end{multline}
where the indicator functions account for events that are explained by
their base rates.

\subsection{Sparsity-Inducing \& Conjugate Priors for Interaction Weights}

We enforce sparsity in the weight matrix by placing a spike-and-slab
prior on the interaction weight matrix, where each edge weight is
sampled from a mixture of a delta function on~$0$ (the spike) and
a distribution over weight intensities.  To represent this prior we introduce a binary adjacency matrix~$\bA$ where $A_{k,k'}=0$ if
neurons $k$ and $k'$ are not connected and $1$ if they are. The
weight intensity matrix is, as before, denoted by~$\bW$. 
With this weight representation, the likelihood becomes:

\begin{multline}
  p(\{s_n,c_n\}^N_{n=1},\bZ\given \lambda_0, \theta, \bA, \bW) =
  \exp\left\{ -\lambda_0 K T \right\} \, \lambda_0^{N-\bone^\trans\bZ\bone}\\
  \times
  \prod_{k=1}^K
  \prod_{n=1}^N
  \exp\left\{
  - A_{c_n,k}W_{c_n,k} \int^T_{s_n} \,g_\theta(\tau - s_n)\,\mathrm{d}\tau
  \right\}
  \prod^N_{n'=1}
  \left[
    A_{c_n,c_{n'}}W_{c_n,c_{n'}}
    \,
    g_\theta(s_{n'} - s_n)
    \right]^{Z_{n,n'}}.
  \label{hawkes_likelihood}
\end{multline}

To express a prior belief that the network has sparsity $\rho$, we
treat each entry in~$\bA$ as a Bernoulli random variable with
parameter~$\rho$. This results in the following prior over $\bA$:

\begin{align}
p(\bA)=\prod_{k=1}^K\prod_{k'=1}^K \rho^{A_{k,k'}}(1-\rho)^{1-A_{k,k'}}.
\end{align}

The entries of ~$\bW$ are conditionally conjugate with a Gamma prior -- 
an elegant and computationally beneficial property that arises from
our decision to separate the weights from the impulse response shape. 
The prior  ~$W_{k,k'} \sim \distGamma(\alpha^{0}_{k,k'},
\beta^{0}_{k,k'})$ results in parameter updates:

\begin{align}
  W_{k,k'}\given \theta, \{s_n,c_n\}^N_{n=1},\bZ &\sim
  \distGamma(\alpha_{k,k'}, \beta_{k,k'}),\\ \alpha_{k,k'} &=
  \alpha^0_{k,k'} + \sum_{n=1}^N\sum_{n'=1}^N
  \delta_{c_n,k}\delta_{c_{n'},k'} Z_{n,n'},\\ \beta_{k,k'} &=
  \beta^0_{k,k'} + \sum_{n=1}^N
  \delta_{c_n,k}\int^T_{s_n}g_\theta(\tau-s_n)\,\mathrm{d}\tau.
\end{align}

Here and elsewhere, we use the inverse-scale parameterization
\begin{align}
  \distGamma(x\given\alpha,\beta) &=
  \frac{\beta^\alpha}{\Gamma(\alpha)}
  x^{\alpha-1}
  \exp\{-\beta\,x\}.
\end{align}

\subsection{Conjugate priors for base rates and kernels.}

Conditioned on the latent causes~$\bZ$ of each event, Gamma priors are
also conditionally conjugate for the node base rates. Some families of kernels
also yield conditionally conjugate priors.

In particular, suppose that each base rate~${\lambda^{(k)}_0 \sim \distGamma(
\alpha^{0}_{\lambda}, \beta^{0}_{\lambda})}$, then the conditional
posterior update for the base rate is 
\begin{align}
  \lambda^{(k)}_0\given \bZ
  &\sim \distGamma( \alpha^0_{\lambda} + \sum_n \bbI( c_n = k ) \bbI( \sum_{n'}Z_{n',n} = 0),\;
  \beta + T).
\end{align}

We also introduce Gamma priors for the kernel parameters. For
computational efficiency we make the simplifying assumption that
kernels are non-zero only over the bounded domain~$[0,t_{max}]$. We
truncate a kernel by multiplying it by a step function and
renormalizing. Furthermore, we make the approximating assumption that the
data truly extends from~$[0,T+t_{max}]$ to ensure that each kernel integrates to
one over the simulation. Each event on node~$i$ then induces~$W_{i,j}$
events on neuron $j$ in expectation. This does penalize events at the
very end of the dataset for inducing fewer than expected events, but
the effect will be minimal for~$t_{max}\ll T$.

Modeling kernels~$g_\theta(t)$ as 
truncated exponential distributions of the form
\begin{align}
  g_\theta(t) &= \frac{1}{G_\theta(t_{max})} \theta\,\exp\{-\theta t\},\\
 G_\theta(t_{max})&=1-\exp\{-\theta t_{max}\},\\
  \theta &\sim \distGamma(\alpha^0_\theta, \beta^0_\theta),
\end{align}
 leads to updates
\begin{align}
  \theta\given \bZ, \{s_n, c_n\}^N_{n=1}
  &\sim \distGamma(\alpha_\theta, \beta_\theta)\\
  \alpha_\theta &= \alpha^0_\theta + \bone^\trans\bZ\bone\\
  \beta_\theta &= \beta^0_\theta + \sum_{n=1}^N\sum_{n'=1}^N
  Z_{n,n'}(s_{n'} - s_{n}).
\end{align}

Alternatively we could assume that $g_\theta(t)$ is a truncated log-normal distribution. This
distribution is more flexible and able to capture temporal delays in the kernels, while still
providing conjugate updates. The model
\begin{align}
g_{\mu,\tau}(t)&=\frac{1}{G_{\mu,\tau}(t_{max})}\frac{\sqrt{\tau}}{t\sqrt{2\pi}}
\exp\{\frac{-\tau}{2}(\ln t-\mu)^2\}\\
G_{\mu,\tau}&=\frac{1+\text{erf}\{\sqrt{\frac{\tau}{2}}(\ln t_{max }-\mu)\}}{2}\\
\mu &\sim \distNormal(\mu_\mu^0,\tau_\mu^0)\\
\tau &\sim \distGamma(\alpha_{\tau}^0,\beta_{\tau}^0)
\end{align}
leads to updates
\begin{align}
\mu\given \bZ, \{s_n, c_n\}^N_{n=1}, \tau &\sim \distNormal(\mu_\mu,\tau_\mu)\\
\mu_\mu &= \frac{\tau_\mu^0\mu_\mu^0 + \tau\big[ \sum_{n=1}^N\sum_{n'=1}^N
  Z_{n,n'}\ln(s_{n'} - s_{n})\big]}{\tau_\mu^0+\tau(\bone^\trans\bZ\bone)}\\
\tau_\mu &= \tau_\mu^0+\tau(\bone^\trans\bZ\bone)
\end{align}
and
\begin{align}
\tau\given \bZ, \{s_n, c_n\}^N_{n=1}, \mu &\sim \distGamma(\alpha_\tau,\beta_\tau)\\
\alpha_\tau &= \alpha_\tau^0+\frac{1}{2}(\bone^\trans\bZ\bone)\\
\beta_\tau &= \beta_\tau^0+\frac{1}{2}\sum_{n=1}^N\sum_{n'=1}^N
  Z_{n,n'}(\ln(s_{n'} - s_{n})-\mu)^2.
\end{align}

\section{Markov Chain Monte Carlo for Sparse Multivariate Hawkes Processes}

Our goal is to produce samples from the posterior distribution of the
model parameters given event observations $\{s_n, c_n\}^N_{n=1}$:

\begin{multline}
  p(\{\lambda^{(k)}_0\}^K_{k=1},\theta, \bW, \bZ, \bA \given \{s_n, c_n\}^N_{n=1}) 
  \propto p(\{s_n, c_n\}^N_{n=1}, \bZ | \{\lambda^{(k)}_0\}^K_{k=1},\theta, \bW, \bA)\\
  \times p(\bW \given \alpha^0_W, \beta^0_W) p(\bA \given \rho) p(\theta \given \alpha_\theta, \beta_\theta)
  p(\{\lambda^{(k)}_0\}^K_{k=1})
\end{multline} 

If $A_{k,k'}=0$ then the conditional distribution on the weight
entry $W_{k,k'}$ reduces to the prior. Additionally, the likelihood is zero
if ~$\bZ$ assigns parent spikes on disconnected nodes. 
This makes Gibbs sampling of $\bA$ and $\bZ$ separately prohibitively slow since an entry in $\bA$
can be set to zero only if the current assignment of $\bZ$ does not
rely on the entry. The probability of this happening shrinks rapidly with the
size of the dataset.

To circumvent this problem we introduce a Metropolis-Hastings operator
to sample $\bA$ and $\bZ$ in unison. Our operator will randomly choose
an entry $A_{k,k'}$ and propose to set it to $1$ with probability
$\gamma$ or to $0$ with probability $1-\gamma$, regardless of its
current value. It will simultaneously propose to reassign the parents
of spikes on node $k'$ according to their conditional probabilities
under the new matrix $\bA^*$. 

Since we draw $\bZ^*$ from its conditional distribution, which is in
turn proportional to the likelihood, many terms cancel out in the
acceptance probability. In fact, if the entry $A_{k,k'}$ is already
equal to the proposed value, then the likelihoods cancel to leave
\begin{align}
P_{add} = \min\bigg(1,\frac{1-\gamma}{\gamma}\bigg), \quad
P_{del} = \min\bigg(1,\frac{\gamma}{1-\gamma}\bigg)
\end{align}
This is to be expected since our proposals when $\bA$ does not change
are drawn exactly from the distribution we wish to sample.

If the proposal changes the value of $A_{k,k'}$ then we will have residual terms corresponding
to the change in likelihood with or without the connection. 
\begin{align}
\nonumber P_{add} = \min&\bigg(1,\;\exp\big\{-\sum_{n=1}^N \bbI( c_n = k)W_{k,k'}\big\}
\times \frac{\rho}{1-\rho} \\
&\times\frac{1-\gamma}{\gamma} \times \prod_{n'=1}^N \bigg[
\frac{\lambda_0^{k'}+\sum_{n=1}^{n'-1}\bA^*_{c_n,c_{n'}}\bW_{c_n,c_{n'}}g_\theta(s_{n'}-s_n)}
{\lambda_0^{k'}+\sum_{n=1}^{n'-1}\bA_{c_n,c_{n'}}\bW_{c_n,c_{n'}}g_\theta(s_{n'}-s_n)}
\bigg]^{\bbI( c_{n'} = k')}
\bigg)
\end{align}

%\begin{align}
%\nonumber P_{del} =  \min\bigg(1,\;&\exp\big\{\sum_{n=1}^N \delta_{c_n,k}W_{k,k'}\big\}
%\times \frac{1-\rho}{\rho} \\
%&\quad\times\frac{\gamma}{1-\gamma} \times \prod_{n'=1}^N
%\frac{\lambda_0+\sum_{n=1}^{n'-1}\bA^*_{c_n,c_{n'}}\bW_{c_n,c_{n'}}g_\theta(s_{n'}-s_n)}
%{\lambda_0+\sum_{n=1}^{n'-1}\bA_{c_n,c_{n'}}\bW_{c_n,c_{n'}}g_\theta(s_{n'}-s_n)}
%\bigg)
%\end{align}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/new_synthetic}
\caption{\textit{Left:} True weight matrix for a synthetic dataset with 10 nodes. 
\textit{Center:} Inferred weight matrix averaged over 1000 MCMC samples after
20K burn-in iterations. \textit{Right:} ROC of inferred vs. true sparsity structure 
computed by thresholding $\bW_{approx}$ at decreasing levels. Our model performs
almost exactly for the 10 node network, and on a 100-node, 1M-event dataset it 
performs consistently better than chance after 1M burn-in iterations.}
\label{fig:sim_weights}
\end{figure}


$P_{del}$ shares the same form, but the first three terms are inverted and
~$\bA^*_{k,k'}$ is zero instead of one. 
The likelihood and proposal ratios counteract one another to ensure the
acceptance probability is proportional to the evidence for the proposed entry.
Note, also, that the acceptance probability does not
depend on the actual parents chosen. Therefore the decision to accept
can be made before the potentially expensive task of selecting parents
is performed.

\section{Experimental Results}
To validate our sparse network discovery method and explore its
performance on real~world problems, we apply our inference algorithm
to simulated and real datasets. 

\subsection{Recovering the Networks of Simulated Datasets}
We generated datasets of simulated spikes from excitatory networks
with known parameters. These networks were simulated from the
multivariate Hawkes proccess described in
equation~\ref{hawkes_likelihood}. Events were simulated in a recursive
manner, i.e., the first generation of events was generated from a
Poisson process with each node's base rate, and each event recursively
generated subsequent generations of Poisson processes on the network
according to the interaction matrix; note that other methods of
varying accuracy and efficiency exist for simulating Hawkes processes
\cite{Moller2005}. 

We simulated one network with 10 nodes and one with 100 nodes. In Figure~\ref{fig:sim_weights}, we compare
the true weight matrix for the simulated data to the expected weight
matrix recovered by our inference algorithm. For small datasets the 
inference algorithm quickly recovers the underlying weights, sparsity structure,
and kernel parameters within thousands of burnin iterations. As discussed
 in section  5, this takes very little time on a GPU. True parameters of larger
datasets, such as our synthetic dataset of 100 nodes and 1M spikes, can 
be recovered but at the cost of a greater number of MCMC iterations. For example,
the results shown in the ROC curve of Figure ~\ref{fig:sim_weights} reflects
results after 1M burnin iterations. On a GPU our simulations completed in 4 hours.

\subsection{Discovering Functional Structure from Neural Spike Trains}
Neural spiking data is a motivating example for our sparse network
discovery algorithm; we therefore applied our method to a dataset of
neural spike trains from ten neurons recorded in the visual cortex of
an anethesized cat. Neural data were recorded by Tim Blanche in the
laboratory of Nicholas Swindale, University of British Columbia, and
downloaded from the NSF-funded CRCNS Data Sharing website. For this
dataset, we clamp the self-excitatory edges to zero to express the
intuition that neurons do not self-excite. 

To assess the robustness of our algorithm, we inferred the network
structure separately using data from two different experiments: in the
first, the animal viewed a drifting bar and, in the second, the animal
viewed natural movies. Figure~\ref{fig:neural_dataset} compares the
average weight matrices discovered on each of the datasets; these
matrices have a Pearson correlation of $\rho=.65$. Further, the
inferred shape of the excitatory kernel corresponds well to our
intuition about neural dynamics, including a $<10$ ms delay that could
reflect action potential conduction delays within and between cortical
regions. This kernel shape is discovered even though the
log-normal kernel prior is quite vague and places much of its mass on
kernel shapes that are very different from the discovered shape.


\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/neural}
\caption{\textit{Left:} Comparison of inferred weight matrices for 10
neurons in the visual cortex in response to two different stimuli: a drifting bar
and a natural movie. The matrices each capture many similar relationships between
pairs of neurons, with a correlation coefficient of 0.65. 
\textit{Right:} The inferred impulse response for both stimuli is a log-normal 
with a short delay. Despite a vague prior on the kernel parameters, the inferred
values are nearly identical for both stimuli.}
\label{fig:neural_dataset}
\end{figure}

\subsection{Inferring Relationships Between Financial Stocks}

Hawkes processes have been used to model the movements of financial
instruments \cite{Embrechts2011}, but these models have been limited
to small numbers of event streams and have relied on maximum
likelihood estimation algorithms. Here, we use our algorithm to
discover the interaction structure in a network in which each node
represent the uptick or downtick of a given financial or technology
sector in the S\&P 100 stock index. 

We ran our algorithm on a small~$20$ node network consisting of~$5$
nodes from each of the technology and finance
sectors~\ref{fig:small_finance}. The stocks are sorted by sector on
each access. The algorithm reveals a cluster of connections among the
finance stocks but not among the technology stocks. To assess the
scaling of our algorithm, we applied our method to all~$30$ stocks
within the finance and technology sectors of the S\&P~100, resulting
in a network of~$60$ nodes~\ref{fig:big_finance}. Note that we find
the same by~sector clustering within the weight matrix.

\begin{figure}
\centering
\subfloat[10 stocks]{
\label{fig:small_finance}
\includegraphics[width=.3\textwidth]{figures/new_financeGrid}
}~
\subfloat[30 stocks]{
\label{fig:big_finance}
\includegraphics[width=.6\textwidth]{figures/bigfinance}
}
\caption{Inferred structure in a network of 10 stocks (\textit{left}), and 30 stocks (\textit{right}) 
half from the finance
sector and half from the technology sector. Each stock is modeled by two 
processes, one for upticks and one for downticks in price. In the upper row,
the influence of upticks on upticks (left) and downticks (right) is shown. The
lower row of matrices shows the influence of downticks on upticks (left) and 
other downticks (right). Each matrix is divided into quadrants showing the influence
between financial and technology stocks. In all cases financial stocks have a 
strong influence on one another over the short time scales of the exponential
kernel used, regardless of whether the event is an uptick or downtick. Relationships
among technology stocks and across sectors appears to be less prevalent over 
similar timescales.
Similar patterns appear in both the 10- and 30-stock datasets, though the inferred
weights for the larger dataset are generally weaker and the overal sparsity is lower. 
}
\end{figure}

\section{Discussion}

In this paper, we have described a method for discovering directed network
structure in spiking datasets: we introduce a conjugate prior
formulation of the multivariate Hawkes process and a sparsity
promoting prior over network connectivity. Our Markov chain Monte
Carlo algorithm for performing inference over this model reveals
interpretable network structure in a variety of problems.

Although methods exist for learning Hawkes model parameters
\cite{Simma2010, Alberta1992, Ozaki1979a}, we provide the first fully
Bayesian characterization this model. Moreover, our model explicitly
builds in the intuition that networks of interactions are sparsely
connected. In addition to a sparse connectivity structure, our model
provides a characterization of the temporal influence of one node on
another. Although in each model application here, we have chosen a single class
of functions for the temporal kernel $g_\theta$, an important
extension of this work will be to maintain a mixture over kernels,
learning the appropriate kernel family from data.

Our inference algorithm is amenable to parallel computation: updates
to the latent causes $\bZ$ and to the entries in weight matrix $\bW$
are independent of one another. The largest network we analyzed
contained 60 nodes with several thousand events, and, using a NVIDIA
Fermi GPU, we were able obtain qualitatively good mixing of our MCMC
chains within ten minutes. As with all MCMC algorithms, however,
assessing how well our Markov chains mix can be problematic. That said, given the
speed and parallizability of the algorithm and the promising results
of our experiments, we believe that this is a promising method for
discovering a sparse structure for large networks of spiking events.


\bibliographystyle{plain} 
\bibliography{library}

\end{document}
